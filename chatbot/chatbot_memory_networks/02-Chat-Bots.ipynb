{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___\n",
    "# Question and Answer Chat Bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will be working with the Babi Data Set from Facebook Research.\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Format of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setting up Vocabulary of All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set that holds the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    # In case you don't know what a union of sets is:\n",
    "    # https://www.programiz.com/python-programming/methods/set/union\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for pad_sequences\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discarded': 1,\n",
       " 'sandra': 2,\n",
       " 'no': 3,\n",
       " 'to': 4,\n",
       " 'got': 5,\n",
       " 'went': 6,\n",
       " 'garden': 7,\n",
       " 'football': 8,\n",
       " 'travelled': 9,\n",
       " 'bathroom': 10,\n",
       " 'the': 11,\n",
       " 'there': 12,\n",
       " 'took': 13,\n",
       " 'put': 14,\n",
       " 'daniel': 15,\n",
       " 'milk': 16,\n",
       " 'moved': 17,\n",
       " 'yes': 18,\n",
       " 'mary': 19,\n",
       " 'is': 20,\n",
       " 'grabbed': 21,\n",
       " '.': 22,\n",
       " 'back': 23,\n",
       " 'journeyed': 24,\n",
       " 'left': 25,\n",
       " 'apple': 26,\n",
       " 'up': 27,\n",
       " 'office': 28,\n",
       " 'down': 29,\n",
       " 'kitchen': 30,\n",
       " 'john': 31,\n",
       " '?': 32,\n",
       " 'picked': 33,\n",
       " 'dropped': 34,\n",
       " 'hallway': 35,\n",
       " 'bedroom': 36,\n",
       " 'in': 37}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalize Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories,Queries,and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
    "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
    "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n",
    "        #\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story,query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 11, 36, 22],\n",
       "       [ 0,  0,  0, ..., 11,  7, 22],\n",
       "       [ 0,  0,  0, ..., 11,  7, 22],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 11, 26, 22],\n",
       "       [ 0,  0,  0, ..., 11,  7, 22],\n",
       "       [ 0,  0,  0, ..., 26, 12, 22]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 31, 37, 11, 30, 32],\n",
       "       [20, 31, 37, 11, 30, 32],\n",
       "       [20, 31, 37, 11,  7, 32],\n",
       "       ...,\n",
       "       [20, 19, 37, 11, 36, 32],\n",
       "       [20,  2, 37, 11,  7, 32],\n",
       "       [20, 19, 37, 11,  7, 32]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0., 503.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 497.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for Inputs\n",
    "\n",
    "Recall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building the Networks\n",
    "\n",
    "To understand why we chose this setup, make sure to read the paper we are using:\n",
    "\n",
    "* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "### Input Encoder m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hanxinle/anaconda2/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/hanxinle/anaconda2/envs/nlp_course/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Input gets embedded to a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# (samples, story_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use dot product to compute the match between first input vector seq and the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add this match matrix with the second input vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 6, 220) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce with RNN (LSTM)\n",
    "answer = LSTM(32)(answer)  # (samples, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hanxinle/anaconda2/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/hanxinle/anaconda2/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 529us/step - loss: 0.9257 - acc: 0.4979 - val_loss: 0.6959 - val_acc: 0.4970\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 334us/step - loss: 0.7076 - acc: 0.4890 - val_loss: 0.6942 - val_acc: 0.4970\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 4s 353us/step - loss: 0.6971 - acc: 0.4966 - val_loss: 0.6943 - val_acc: 0.5030\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 350us/step - loss: 0.6947 - acc: 0.4974 - val_loss: 0.6934 - val_acc: 0.5030\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 3s 298us/step - loss: 0.6940 - acc: 0.5102 - val_loss: 0.6936 - val_acc: 0.4970\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 3s 296us/step - loss: 0.6945 - acc: 0.5112 - val_loss: 0.6937 - val_acc: 0.4970\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 296us/step - loss: 0.6950 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.4970\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 3s 296us/step - loss: 0.6944 - acc: 0.5069 - val_loss: 0.6942 - val_acc: 0.5030\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 3s 295us/step - loss: 0.6945 - acc: 0.5038 - val_loss: 0.6937 - val_acc: 0.4890\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 3s 312us/step - loss: 0.6938 - acc: 0.5096 - val_loss: 0.6937 - val_acc: 0.5010\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.6868 - acc: 0.5372 - val_loss: 0.6708 - val_acc: 0.6870\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.6513 - acc: 0.6001 - val_loss: 0.6069 - val_acc: 0.7680\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.5974 - acc: 0.6524 - val_loss: 0.5299 - val_acc: 0.8210\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 3s 312us/step - loss: 0.5444 - acc: 0.7195 - val_loss: 0.4692 - val_acc: 0.7940\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 4s 420us/step - loss: 0.4985 - acc: 0.7668 - val_loss: 0.4459 - val_acc: 0.8230\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 3s 309us/step - loss: 0.4494 - acc: 0.8055 - val_loss: 0.4536 - val_acc: 0.7760\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 3s 303us/step - loss: 0.4264 - acc: 0.8193 - val_loss: 0.3847 - val_acc: 0.8310\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.4124 - acc: 0.8271 - val_loss: 0.3909 - val_acc: 0.8300\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 3s 297us/step - loss: 0.3984 - acc: 0.8335 - val_loss: 0.4059 - val_acc: 0.8020\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.3890 - acc: 0.8415 - val_loss: 0.3747 - val_acc: 0.8340\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.3774 - acc: 0.8436 - val_loss: 0.3604 - val_acc: 0.8390\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 3s 313us/step - loss: 0.3662 - acc: 0.8500 - val_loss: 0.3580 - val_acc: 0.8430\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.3572 - acc: 0.8511 - val_loss: 0.3558 - val_acc: 0.8500\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.3505 - acc: 0.8525 - val_loss: 0.3485 - val_acc: 0.8310\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.3535 - acc: 0.8514 - val_loss: 0.3512 - val_acc: 0.8420\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.3400 - acc: 0.8544 - val_loss: 0.3416 - val_acc: 0.8360\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.3309 - acc: 0.8553 - val_loss: 0.3408 - val_acc: 0.8410\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.3304 - acc: 0.8581 - val_loss: 0.3430 - val_acc: 0.8290\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 5s 471us/step - loss: 0.3300 - acc: 0.8537 - val_loss: 0.3587 - val_acc: 0.8420\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 3s 303us/step - loss: 0.3276 - acc: 0.8598 - val_loss: 0.3458 - val_acc: 0.8270\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 3s 298us/step - loss: 0.3200 - acc: 0.8617 - val_loss: 0.3518 - val_acc: 0.8430\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.3177 - acc: 0.8611 - val_loss: 0.3557 - val_acc: 0.8440\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 3s 299us/step - loss: 0.3191 - acc: 0.8621 - val_loss: 0.3424 - val_acc: 0.8250\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 3s 332us/step - loss: 0.3149 - acc: 0.8623 - val_loss: 0.3444 - val_acc: 0.8420\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 4s 414us/step - loss: 0.3149 - acc: 0.8600 - val_loss: 0.3418 - val_acc: 0.8430\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 4s 431us/step - loss: 0.3131 - acc: 0.8610 - val_loss: 0.3346 - val_acc: 0.8380\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 4s 413us/step - loss: 0.3119 - acc: 0.8599 - val_loss: 0.3404 - val_acc: 0.8370\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 4s 409us/step - loss: 0.3126 - acc: 0.8575 - val_loss: 0.3417 - val_acc: 0.8310\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 4s 410us/step - loss: 0.3136 - acc: 0.8614 - val_loss: 0.3401 - val_acc: 0.8460\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 4s 402us/step - loss: 0.3120 - acc: 0.8622 - val_loss: 0.3563 - val_acc: 0.8390\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 4s 371us/step - loss: 0.3100 - acc: 0.8643 - val_loss: 0.3702 - val_acc: 0.8370\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 4s 397us/step - loss: 0.3108 - acc: 0.8620 - val_loss: 0.3556 - val_acc: 0.8360\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 4s 389us/step - loss: 0.3104 - acc: 0.8612 - val_loss: 0.3568 - val_acc: 0.8380\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.3083 - acc: 0.8625 - val_loss: 0.3469 - val_acc: 0.8490\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3016 - acc: 0.8640 - val_loss: 0.3448 - val_acc: 0.8310\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 4s 362us/step - loss: 0.3086 - acc: 0.8621 - val_loss: 0.3371 - val_acc: 0.8430\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 4s 361us/step - loss: 0.3029 - acc: 0.8674 - val_loss: 0.3429 - val_acc: 0.8440\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 0.3072 - acc: 0.8636 - val_loss: 0.3627 - val_acc: 0.8320\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.3053 - acc: 0.8630 - val_loss: 0.3348 - val_acc: 0.8450\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 4s 357us/step - loss: 0.3015 - acc: 0.8660 - val_loss: 0.3495 - val_acc: 0.8300\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 4s 355us/step - loss: 0.3018 - acc: 0.8648 - val_loss: 0.3445 - val_acc: 0.8450\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 4s 364us/step - loss: 0.3011 - acc: 0.8653 - val_loss: 0.3466 - val_acc: 0.8390\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 0.3012 - acc: 0.8664 - val_loss: 0.3487 - val_acc: 0.8440\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 4s 365us/step - loss: 0.3027 - acc: 0.8670 - val_loss: 0.3456 - val_acc: 0.8280\n",
      "Epoch 55/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 357us/step - loss: 0.3013 - acc: 0.8653 - val_loss: 0.3431 - val_acc: 0.8360\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 5s 475us/step - loss: 0.2971 - acc: 0.8658 - val_loss: 0.3493 - val_acc: 0.8320\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 5s 521us/step - loss: 0.2954 - acc: 0.8675 - val_loss: 0.3728 - val_acc: 0.8210\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 5s 533us/step - loss: 0.2968 - acc: 0.8672 - val_loss: 0.3381 - val_acc: 0.8360\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.2933 - acc: 0.8702 - val_loss: 0.3418 - val_acc: 0.8420\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 3s 344us/step - loss: 0.2937 - acc: 0.8692 - val_loss: 0.3569 - val_acc: 0.8390\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2969 - acc: 0.8658 - val_loss: 0.3664 - val_acc: 0.8360\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.2932 - acc: 0.8685 - val_loss: 0.3505 - val_acc: 0.8330\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.2945 - acc: 0.8685 - val_loss: 0.3613 - val_acc: 0.8380\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.2930 - acc: 0.8688 - val_loss: 0.3579 - val_acc: 0.8270\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2947 - acc: 0.8692 - val_loss: 0.3469 - val_acc: 0.8410\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.2922 - acc: 0.8680 - val_loss: 0.3508 - val_acc: 0.8300\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.2935 - acc: 0.8675 - val_loss: 0.3533 - val_acc: 0.8350\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2864 - acc: 0.8689 - val_loss: 0.3684 - val_acc: 0.8370\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2906 - acc: 0.8724 - val_loss: 0.3809 - val_acc: 0.8290\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.2921 - acc: 0.8689 - val_loss: 0.3602 - val_acc: 0.8380\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.2886 - acc: 0.8684 - val_loss: 0.3505 - val_acc: 0.8300\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.2911 - acc: 0.8717 - val_loss: 0.3666 - val_acc: 0.8310\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 4s 407us/step - loss: 0.2909 - acc: 0.8703 - val_loss: 0.3675 - val_acc: 0.8380\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 4s 407us/step - loss: 0.2917 - acc: 0.8712 - val_loss: 0.3636 - val_acc: 0.8320\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2909 - acc: 0.8705 - val_loss: 0.3611 - val_acc: 0.8370\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 4s 354us/step - loss: 0.2921 - acc: 0.8686 - val_loss: 0.4053 - val_acc: 0.8360\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 4s 367us/step - loss: 0.2906 - acc: 0.8713 - val_loss: 0.3618 - val_acc: 0.8280\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.2892 - acc: 0.8705 - val_loss: 0.3621 - val_acc: 0.8300\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 3s 342us/step - loss: 0.2871 - acc: 0.8722 - val_loss: 0.3732 - val_acc: 0.8330\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 3s 349us/step - loss: 0.2901 - acc: 0.8727 - val_loss: 0.3758 - val_acc: 0.8320\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 3s 347us/step - loss: 0.2866 - acc: 0.8748 - val_loss: 0.4205 - val_acc: 0.8290\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 4s 379us/step - loss: 0.2852 - acc: 0.8732 - val_loss: 0.3865 - val_acc: 0.8240\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 3s 339us/step - loss: 0.2930 - acc: 0.8700 - val_loss: 0.3783 - val_acc: 0.8360\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 4s 351us/step - loss: 0.2849 - acc: 0.8720 - val_loss: 0.3559 - val_acc: 0.8310\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 305us/step - loss: 0.2840 - acc: 0.8732 - val_loss: 0.4081 - val_acc: 0.8370\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 3s 314us/step - loss: 0.2879 - acc: 0.8714 - val_loss: 0.3642 - val_acc: 0.8310\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.2821 - acc: 0.8754 - val_loss: 0.3785 - val_acc: 0.8340\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.2791 - acc: 0.8747 - val_loss: 0.3810 - val_acc: 0.8310\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 3s 299us/step - loss: 0.2774 - acc: 0.8772 - val_loss: 0.3724 - val_acc: 0.8410\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.2778 - acc: 0.8789 - val_loss: 0.3880 - val_acc: 0.8170\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 4s 351us/step - loss: 0.2800 - acc: 0.8777 - val_loss: 0.3608 - val_acc: 0.8350\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 4s 405us/step - loss: 0.2801 - acc: 0.8788 - val_loss: 0.3753 - val_acc: 0.8310\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 4s 353us/step - loss: 0.2768 - acc: 0.8774 - val_loss: 0.3805 - val_acc: 0.8250\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 3s 347us/step - loss: 0.2774 - acc: 0.8777 - val_loss: 0.3824 - val_acc: 0.8270\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 3s 342us/step - loss: 0.2773 - acc: 0.8772 - val_loss: 0.3867 - val_acc: 0.8350\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.2822 - acc: 0.8771 - val_loss: 0.3661 - val_acc: 0.8300\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 0.2825 - acc: 0.8774 - val_loss: 0.3883 - val_acc: 0.8350\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 3s 338us/step - loss: 0.2840 - acc: 0.8772 - val_loss: 0.3991 - val_acc: 0.8330\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 4s 402us/step - loss: 0.2730 - acc: 0.8794 - val_loss: 0.3900 - val_acc: 0.8210\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.2762 - acc: 0.8786 - val_loss: 0.3765 - val_acc: 0.8370\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.2745 - acc: 0.8829 - val_loss: 0.4867 - val_acc: 0.8300\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 3s 334us/step - loss: 0.2750 - acc: 0.8827 - val_loss: 0.4321 - val_acc: 0.8240\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 3s 336us/step - loss: 0.2721 - acc: 0.8820 - val_loss: 0.4351 - val_acc: 0.8280\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2729 - acc: 0.8826 - val_loss: 0.4235 - val_acc: 0.8300\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.2717 - acc: 0.8850 - val_loss: 0.3903 - val_acc: 0.8330\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.2683 - acc: 0.8883 - val_loss: 0.4217 - val_acc: 0.8320\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 4s 352us/step - loss: 0.2685 - acc: 0.8855 - val_loss: 0.3985 - val_acc: 0.8350\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 4s 353us/step - loss: 0.2722 - acc: 0.8871 - val_loss: 0.4612 - val_acc: 0.8280\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 4s 363us/step - loss: 0.2583 - acc: 0.8862 - val_loss: 0.3972 - val_acc: 0.8280\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 3s 334us/step - loss: 0.2642 - acc: 0.8863 - val_loss: 0.4610 - val_acc: 0.8380\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 5s 485us/step - loss: 0.2625 - acc: 0.8861 - val_loss: 0.4196 - val_acc: 0.8300\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 6s 573us/step - loss: 0.2660 - acc: 0.8859 - val_loss: 0.4063 - val_acc: 0.8290\n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 540us/step - loss: 0.2668 - acc: 0.8852 - val_loss: 0.4431 - val_acc: 0.8270\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 4s 351us/step - loss: 0.2582 - acc: 0.8941 - val_loss: 0.4659 - val_acc: 0.8270\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 4s 360us/step - loss: 0.2613 - acc: 0.8889 - val_loss: 0.4473 - val_acc: 0.8290\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.2625 - acc: 0.8873 - val_loss: 0.4373 - val_acc: 0.8270\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 3s 313us/step - loss: 0.2642 - acc: 0.8919 - val_loss: 0.4491 - val_acc: 0.8220\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.2627 - acc: 0.8916 - val_loss: 0.4563 - val_acc: 0.8350\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.2590 - acc: 0.8911 - val_loss: 0.4731 - val_acc: 0.8360\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.2538 - acc: 0.8924 - val_loss: 0.4785 - val_acc: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chatbot_120_epochs.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "### Plotting Out Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Given Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filename)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'got',\n",
       " 'the',\n",
       " 'milk',\n",
       " 'there',\n",
       " '.',\n",
       " 'John',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary got the milk there . John moved to the bedroom .\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[0][0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is John in the kitchen ?\n"
     ]
    }
   ],
   "source": [
    "query = ' '.join(word for word in test_data[0][1])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Test Answer from Data is: no\n"
     ]
    }
   ],
   "source": [
    "print(\"True Test Answer from Data is:\",test_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Your Own Stories and Questions\n",
    "\n",
    "Remember you can only use words from the existing vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'left',\n",
       " 'the',\n",
       " 'kitchen',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'dropped',\n",
       " 'the',\n",
       " 'football',\n",
       " 'in',\n",
       " 'the',\n",
       " 'garden',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the whitespace of the periods\n",
    "my_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\n",
    "my_story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"Is the football in the garden ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'the', 'football', 'in', 'the', 'garden', '?']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(),my_question.split(),'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story,my_ques,my_ans = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9556777\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
